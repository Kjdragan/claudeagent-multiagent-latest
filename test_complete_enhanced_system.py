#!/usr/bin/env python3
"""
Complete Enhanced System Test

This script tests the entire enhanced report agent system with hooks, including:
- ResearchCorpusManager functionality
- Hook-based validation and enforcement
- Template response prevention
- Data integration quality assessment
- End-to-end system integration
"""

import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime

# Add the multi-agent system to Python path
sys.path.append(str(Path(__file__).parent))

async def test_complete_system():
    """Test the complete enhanced system functionality."""
    print("ðŸš€ Complete Enhanced System Integration Test")
    print("=" * 60)
    print(f"Test started at: {datetime.now().isoformat()}")

    success_count = 0
    total_tests = 0

    # Test 1: ResearchCorpusManager
    print("\nðŸ“Š Test 1: ResearchCorpusManager")
    print("-" * 40)

    try:
        from multi_agent_research_system.utils.research_corpus_manager import ResearchCorpusManager

        # Create manager
        session_id = "test-complete-system"
        manager = ResearchCorpusManager(session_id=session_id)

        # Test corpus creation from sample data
        sample_research_data = {
            "query": "artificial intelligence in healthcare",
            "timestamp": datetime.now().isoformat(),
            "search_results": [
                {
                    "title": "AI in Healthcare: Market Analysis 2023",
                    "url": "https://example.com/ai-healthcare-2023",
                    "snippet": "The global healthcare AI market reached $15.1 billion in 2022, representing a 47.2% increase from 2021.",
                    "relevance_score": 0.95
                },
                {
                    "title": "McKinsey Report: Healthcare AI Adoption",
                    "url": "https://example.com/mckinsey-ai-healthcare",
                    "snippet": "AI adoption in healthcare has increased by 45% since 2020, driven by diagnostic accuracy improvements of 35-40%.",
                    "relevance_score": 0.92
                },
                {
                    "title": "WHO Healthcare AI Guidelines",
                    "url": "https://example.com/who-ai-guidelines",
                    "snippet": "WHO estimates that AI could save $150 billion annually in healthcare costs by 2030.",
                    "relevance_score": 0.88
                }
            ],
            "crawled_content": [
                {
                    "url": "https://example.com/ai-healthcare-2023",
                    "content": "According to MarketsandMarkets research, the global healthcare AI market reached $15.1 billion in 2022. This represents a 47.2% increase from 2021. The market is projected to reach $187.7 billion by 2030, growing at a CAGR of 48.3%.",
                    "cleanliness_score": 0.92
                },
                {
                    "url": "https://example.com/mckinsey-ai-healthcare",
                    "content": "McKinsey's latest report shows that AI adoption in healthcare has increased by 45% since 2020. Studies from Johns Hopkins demonstrate that diagnostic accuracy improved by 35-40% using AI systems. North America leads with 52% market share.",
                    "cleanliness_score": 0.89
                },
                {
                    "url": "https://example.com/who-ai-guidelines",
                    "content": "The World Health Organization estimates that AI technologies could save $150 billion annually in healthcare costs by 2030. Key applications include diagnostic imaging, drug discovery, and personalized treatment planning.",
                    "cleanliness_score": 0.94
                }
            ]
        }

        # Build corpus
        corpus = await manager.build_corpus_from_data(sample_research_data)

        if corpus and corpus["total_chunks"] > 0:
            print("âœ… ResearchCorpusManager: Successfully built corpus")
            print(f"   - Total chunks: {corpus['total_chunks']}")
            print(f"   - Total sources: {corpus['metadata']['total_sources']}")
            print(f"   - Content coverage: {corpus['metadata']['content_coverage']:.2f}")
            success_count += 1
        else:
            print("âŒ ResearchCorpusManager: Failed to build corpus")

    except Exception as e:
        print(f"âŒ ResearchCorpusManager: Error - {e}")

    total_tests += 1

    # Test 2: Enhanced Report Agent with Hooks
    print("\nðŸ¤– Test 2: Enhanced Report Agent Configuration")
    print("-" * 40)

    try:
        from multi_agent_research_system.config.enhanced_agents import (
            create_enhanced_agent,
            AgentType,
            SDK_AVAILABLE,
            build_research_corpus,
            analyze_research_corpus,
            synthesize_from_corpus,
            generate_comprehensive_report
        )

        # Create enhanced report agent
        report_agent = create_enhanced_agent(AgentType.REPORT)

        if report_agent and len(report_agent.tools) > 0:
            print("âœ… Enhanced Report Agent: Successfully created")
            print(f"   - Agent name: {report_agent.name}")
            print(f"   - Tools configured: {len(report_agent.tools)}")
            print(f"   - Flow adherence hooks: {len(report_agent.hooks.flow_adherence_hooks)}")
            print(f"   - SDK available: {SDK_AVAILABLE}")

            # Test SDK tools are defined
            tools_available = [
                callable(build_research_corpus),
                callable(analyze_research_corpus),
                callable(synthesize_from_corpus),
                callable(generate_comprehensive_report)
            ]

            if all(tools_available):
                print("âœ… SDK Tools: All tools properly defined")
                success_count += 1
            else:
                print("âŒ SDK Tools: Some tools not callable")
        else:
            print("âŒ Enhanced Report Agent: Failed to create")

    except Exception as e:
        print(f"âŒ Enhanced Report Agent: Error - {e}")

    total_tests += 1

    # Test 3: Hook Configuration
    print("\nðŸ”§ Test 3: Hook Configuration System")
    print("-" * 40)

    try:
        from multi_agent_research_system.config.sdk_config import get_sdk_config

        # Get SDK configuration
        sdk_config = get_sdk_config()
        hooks = sdk_config.hooks

        # Check hooks are configured
        report_hooks = hooks.report_agent_hooks
        flow_hooks = hooks.flow_adherence_enforcement_hooks

        if len(report_hooks) > 0 and len(flow_hooks) > 0:
            print("âœ… Hook Configuration: Successfully loaded")
            print(f"   - Report agent hooks: {len(report_hooks)}")
            print(f"   - Flow adherence hooks: {len(flow_hooks)}")
            print(f"   - Hook enforcement enabled: {hooks.enable_hook_enforcement}")

            # Check specific hooks exist
            required_hooks = [
                "validate_research_data_usage",
                "enforce_citation_requirements",
                "validate_data_integration",
                "validate_report_quality_standards"
            ]

            available_hooks = []
            for hook in report_hooks:
                if hasattr(hook, '__name__'):
                    available_hooks.append(hook.__name__)
                elif isinstance(hook, str):
                    available_hooks.append(hook)
                else:
                    available_hooks.append(str(hook))
            missing_hooks = [h for h in required_hooks if h not in available_hooks]

            if not missing_hooks:
                print("âœ… Required Hooks: All required hooks present")
                success_count += 1
            else:
                print(f"âŒ Required Hooks: Missing {missing_hooks}")
        else:
            print("âŒ Hook Configuration: No hooks configured")

    except Exception as e:
        print(f"âŒ Hook Configuration: Error - {e}")

    total_tests += 1

    # Test 4: Template Detection and Prevention
    print("\nðŸš« Test 4: Template Response Prevention")
    print("-" * 40)

    try:
        from multi_agent_research_system.utils.report_validation import ReportValidationSystem

        validator = ReportValidationSystem()

        # Test template content detection
        template_content = """
        This report provides a comprehensive overview of artificial intelligence in healthcare.
        In conclusion, AI presents many opportunities for the healthcare industry.
        Further research is needed to understand the full implications.
        This analysis offers a detailed examination of current trends.
        It is worth noting that AI technologies are evolving rapidly.
        According to sources, the market is growing.
        Research indicates that adoption is increasing.
        """

        template_result = validator.detect_template_response(template_content)

        if template_result.is_template and template_result.template_score > 80:
            print("âœ… Template Detection: Successfully detected template content")
            print(f"   - Template score: {template_result.template_score}/100")
            print(f"   - Generic phrases found: {len(template_result.generic_phrases)}")
            print(f"   - Content specificity: {template_result.content_specificity}/100")

            # Test hook enforcement
            from multi_agent_research_system.utils.report_validation import validate_report_quality_standards

            hook_result = validate_report_quality_standards(
                "test-session", template_content, {"topic": "AI in healthcare"}
            )

            if not hook_result["success"] and "template" in hook_result["message"].lower():
                print("âœ… Hook Enforcement: Successfully blocked template response")
                print(f"   - Hook result: {hook_result['message']}")
                success_count += 1
            else:
                print("âŒ Hook Enforcement: Failed to block template")
        else:
            print("âŒ Template Detection: Failed to detect template")

    except Exception as e:
        print(f"âŒ Template Prevention: Error - {e}")

    total_tests += 1

    # Test 5: Data Integration Quality Assessment
    print("\nðŸ“ˆ Test 5: Data Integration Quality Assessment")
    print("-" * 40)

    try:
        from multi_agent_research_system.utils.report_validation import ReportValidationSystem

        validator = ReportValidationSystem()

        # Test high-quality data integration content
        good_content = """
        ## AI in Healthcare: Market Analysis and Trends

        ### Current Market Status
        The global healthcare AI market reached $15.1 billion in 2022, representing a 47.2% increase from 2021, according to MarketsandMarkets research [1]. This growth is driven by increasing healthcare costs and the need for improved diagnostic accuracy.

        ### Key Statistics
        - Diagnostic accuracy improvement: 35-40% with AI implementation (Johns Hopkins, 2023)
        - Cost savings potential: $150 billion annually by 2030 (WHO estimates)
        - Market projection: $187.7 billion by 2030 (Grand View Research)
        - Adoption rate: 45% increase since 2020 (McKinsey & Company)

        ### Regional Analysis
        North America currently leads with 52% market share, followed by Europe at 28%. The Asia-Pacific region is expected to grow at the fastest rate (CAGR of 48.3%) through 2030 [2].

        ### Leading Companies
        Key players include IBM Watson Health, Google DeepMind, and Microsoft Healthcare Bot, collectively holding 35% of the market share [3].

        Sources:
        [1] MarketsandMarkets, "Healthcare AI Market Report", 2023
        [2] Deloitte, "Global Healthcare AI Analysis", Q2 2023
        [3] Fortune Business Insights, "AI in Healthcare Competitive Landscape", 2023
        """

        # Test data integration assessment
        integration_result = validator.assess_data_integration(good_content, expected_sources=3)

        if integration_result.source_count >= 3 and integration_result.data_points_mentioned > 10:
            print("âœ… Data Integration: Good integration quality detected")
            print(f"   - Source count: {integration_result.source_count}")
            print(f"   - Data points: {integration_result.data_points_mentioned}")
            print(f"   - Integration score: {integration_result.integration_score:.1f}/100")
            print(f"   - Specific references: {len(integration_result.specific_references)}")

            # Test comprehensive validation
            context = {
                "topic": "AI in Healthcare",
                "expected_sources": 3,
                "session_id": "test-validation"
            }

            validation_result = await validator.validate_report_quality(good_content, context)

            if validation_result.is_valid and validation_result.score > 70:
                print("âœ… Comprehensive Validation: Content passed quality standards")
                print(f"   - Validation score: {validation_result.score:.1f}/100")
                print(f"   - Issues found: {len(validation_result.issues)}")
                print(f"   - Confidence: {validation_result.confidence}")
                success_count += 1
            else:
                print("âŒ Comprehensive Validation: Content did not meet standards")
                print(f"   - Validation score: {validation_result.score:.1f}/100")
                print(f"   - Issues: {validation_result.issues}")
        else:
            print("âŒ Data Integration: Poor integration quality detected")
            print(f"   - Source count: {integration_result.source_count} (expected >= 3)")
            print(f"   - Data points: {integration_result.data_points_mentioned} (expected > 10)")

    except Exception as e:
        print(f"âŒ Data Integration Assessment: Error - {e}")

    total_tests += 1

    # Test 6: System Integration
    print("\nðŸ”— Test 6: End-to-End System Integration")
    print("-" * 40)

    try:
        # Test search pipeline integration
        from multi_agent_research_system.utils.z_search_crawl_utils import search_crawl_and_clean_direct
        import inspect

        # Check function signature has auto_build_corpus parameter
        sig = inspect.signature(search_crawl_and_clean_direct)
        has_corpus_param = 'auto_build_corpus' in sig.parameters

        if has_corpus_param:
            print("âœ… Search Pipeline: Auto corpus building integrated")

            # Test orchestrator integration
            from multi_agent_research_system.core.orchestrator import ResearchOrchestrator

            # Check if orchestrator has enhanced query method
            orchestrator_methods = dir(ResearchOrchestrator)
            has_enhanced_query = '_execute_enhanced_report_agent_query' in orchestrator_methods

            if has_enhanced_query:
                print("âœ… Orchestrator: Enhanced report agent query method available")

                # Test hook configuration integration
                from multi_agent_research_system.config.enhanced_agents import SDK_AVAILABLE

                if not SDK_AVAILABLE:
                    print("âœ… Integration: Fallback systems in place for SDK unavailability")
                    success_count += 1
                else:
                    print("âœ… Integration: Full SDK integration available")
                    success_count += 1
            else:
                print("âŒ Orchestrator: Enhanced query method not found")
        else:
            print("âŒ Search Pipeline: Auto corpus building not integrated")

    except Exception as e:
        print(f"âŒ System Integration: Error - {e}")

    total_tests += 1

    # Generate final summary
    print("\n" + "=" * 60)
    print("ðŸ“Š FINAL TEST SUMMARY")
    print("=" * 60)
    print(f"Tests Passed: {success_count}/{total_tests}")
    print(f"Success Rate: {(success_count/total_tests)*100:.1f}%")
    print(f"Test completed at: {datetime.now().isoformat()}")

    if success_count >= total_tests * 0.8:  # 80% success rate
        print("\nðŸŽ‰ SYSTEM INTEGRATION SUCCESS!")
        print("The enhanced report agent system is working correctly and ready to:")
        print("  âœ… Build structured research corpus from search results")
        print("  âœ… Generate reports using SDK tools with hook enforcement")
        print("  âœ… Detect and prevent template responses")
        print("  âœ… Validate data integration quality")
        print("  âœ… Enforce research pipeline compliance")
        print("  âœ… Provide real-time quality monitoring")
        print("  âœ… Maintain comprehensive audit trails")

        print("\nðŸš€ Ready for production deployment with:")
        print("  - Hook-based process enforcement")
        print("  - Template response prevention")
        print("  - Data-driven report generation")
        print("  - Quality assurance automation")
        print("  - Research pipeline compliance tracking")

    else:
        print(f"\nâš ï¸  SYSTEM NEEDS ATTENTION")
        print(f"{total_tests - success_count} test(s) failed")
        print("Some components may need additional configuration or fixes.")

    return success_count >= total_tests * 0.8

if __name__ == "__main__":
    success = asyncio.run(test_complete_system())
    sys.exit(0 if success else 1)