"""
Optimized Crawl4AI utility functions with Stage 1 DNS resolution fixes.

This module provides the optimized crawling implementation that resolves the
DNS resolution issues identified in the performance analysis while maintaining
the multi-stage crawling strategy and advanced features.

Key optimizations:
- Fixed Stage 1 cache mode configuration for proper DNS resolution
- Universal CSS selectors that work across all domains
- Smart domain-based cache strategy
- Intelligent anti-bot escalation
- Enhanced performance monitoring
"""

import asyncio
import logging
from datetime import datetime
from typing import List, Optional, Dict, Any
from dataclasses import dataclass
from urllib.parse import urlparse

from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import PruningContentFilter

# Import Logfire configuration
try:
    from config.logfire_config import configure_logfire, is_logfire_available
    if not is_logfire_available():
        configure_logfire(service_name="crawl4ai-optimized")
    import logfire
except ImportError:
    # If logfire config is not available, create a no-op logfire
    class NoOpLogfire:
        def info(self, msg, **kwargs):
            pass
        def warning(self, msg, **kwargs):
            pass
        def error(self, msg, **kwargs):
            pass
        def span(self, msg, **kwargs):
            from contextlib import nullcontext
            return nullcontext()
    logfire = NoOpLogfire()

logger = logging.getLogger(__name__)


@dataclass
class OptimizedCrawlResult:
    """Enhanced result structure with performance metrics."""
    url: str
    success: bool
    content: Optional[str] = None
    error: Optional[str] = None
    duration: float = 0.0
    word_count: int = 0
    char_count: int = 0
    stage_used: str = "unknown"
    cache_mode: str = "unknown"
    anti_bot_level: int = 0
    dns_resolved: bool = False


class OptimizedCrawler:
    """
    Optimized crawler with fixed Stage 1 configuration and intelligent fallback.

    Resolves DNS resolution issues while maintaining performance advantages
    of the multi-stage crawling strategy.
    """

    def __init__(self, browser_configs: Optional[Dict] = None):
        """Initialize with optional browser configurations."""
        self.browser_configs = browser_configs or {}
        self._stats = {
            'total_crawls': 0,
            'successful_crawls': 0,
            'failed_crawls': 0,
            'stage1_successes': 0,
            'stage2_fallbacks': 0,
            'dns_failures': 0,
            'total_duration': 0.0
        }

    def get_cache_mode_for_domain(self, url: str) -> CacheMode:
        """
        Determine optimal cache mode based on domain characteristics.

        Domains with known DNS or anti-bot issues get cache enabled,
        while others can use disabled cache for maximum performance.
        """
        domain = urlparse(url).netloc.lower()

        # Domains known to have DNS or anti-bot issues
        problematic_patterns = [
            'google.com', 'github.com', 'stackoverflow.com',
            'medium.com', 'substack.com', 'youtube.com',
            'facebook.com', 'twitter.com', 'linkedin.com'
        ]

        # Check if domain matches any problematic pattern
        if any(pattern in domain for pattern in problematic_patterns):
            return CacheMode.ENABLED

        # CDNs and cloud platforms often need cache
        cdn_patterns = ['cloudflare', 'fastly', 'akamai', 'amazonaws', 'azureedge']
        if any(pattern in domain for pattern in cdn_patterns):
            return CacheMode.ENABLED

        # Default to enabled for better reliability
        return CacheMode.ENABLED

    def get_universal_css_selectors(self) -> str:
        """
        Return universal CSS selectors that work across most websites.

        Replaces overly specific selectors that were causing failures
        with a more comprehensive set of common content containers.
        """
        return "main, article, .content, .article-body, .post-content, .entry-content, .main-content, .post, .story, .text, #content, #main"

    async def optimized_stage1_crawl(self, url: str) -> OptimizedCrawlResult:
        """
        Optimized Stage 1 crawl with fixed DNS resolution and universal selectors.

        Key improvements:
        - Smart cache mode based on domain analysis
        - Universal CSS selectors for broad compatibility
        - Optimized content filtering thresholds
        - Proper DNS resolution handling
        """
        start_time = datetime.now()

        try:
            with logfire.span("optimized_stage1_crawl", url=url):
                # Smart cache mode configuration
                cache_mode = self.get_cache_mode_for_domain(url)
                logger.info(f"üîß Stage 1 cache mode for {url}: {cache_mode}")

                # Universal CSS selectors for broad compatibility
                universal_selectors = self.get_universal_css_selectors()

                # Optimized crawl configuration
                crawl_config = CrawlerRunConfig(
                    cache_mode=cache_mode,  # ‚úÖ FIXED: Smart cache mode
                    wait_for="body",  # ‚úÖ IMPROVED: Allow proper page initialization
                    css_selector=universal_selectors,  # ‚úÖ FIXED: Universal selectors
                    markdown_generator=DefaultMarkdownGenerator(
                        content_filter=PruningContentFilter(
                            threshold=0.4,  # ‚úÖ IMPROVED: Less aggressive filtering
                            min_word_threshold=50  # ‚úÖ IMPROVED: Preserve longer content blocks
                        )
                    ),
                    page_timeout=30000,  # ‚úÖ IMPROVED: Reasonable timeout
                    delay_before_return_html=0.5  # ‚úÖ IMPROVED: Allow page to settle
                )

                # Execute optimized Stage 1 crawl
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url=url, config=crawl_config)

                # Calculate metrics
                duration = (datetime.now() - start_time).total_seconds()
                content = result.markdown if result.success else None
                word_count = len(content.split()) if content else 0
                char_count = len(content) if content else 0

                # Update statistics
                self._stats['total_crawls'] += 1
                if result.success:
                    self._stats['successful_crawls'] += 1
                    self._stats['stage1_successes'] += 1
                else:
                    self._stats['failed_crawls'] += 1
                    # Check for DNS-specific failures
                    if result.error_message and any(term in result.error_message.lower()
                                                   for term in ['dns', 'name not resolved', 'network']):
                        self._stats['dns_failures'] += 1
                self._stats['total_duration'] += duration

                return OptimizedCrawlResult(
                    url=url,
                    success=result.success,
                    content=content,
                    error=result.error_message if not result.success else None,
                    duration=duration,
                    word_count=word_count,
                    char_count=char_count,
                    stage_used="1_optimized",
                    cache_mode=str(cache_mode),
                    anti_bot_level=0,
                    dns_resolved=result.success
                )

        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds()
            self._stats['total_crawls'] += 1
            self._stats['failed_crawls'] += 1
            self._stats['total_duration'] += duration

            logger.error(f"‚ùå Optimized Stage 1 crawl failed for {url}: {str(e)}")
            return OptimizedCrawlResult(
                url=url,
                success=False,
                error=str(e),
                duration=duration,
                stage_used="1_optimized",
                cache_mode="error",
                anti_bot_level=0,
                dns_resolved=False
            )

    async def intelligent_stage2_fallback(self, url: str, stage1_error: str = None) -> OptimizedCrawlResult:
        """
        Intelligent Stage 2 fallback with context-aware configuration.

        Analyzes Stage 1 failure to determine optimal Stage 2 approach.
        """
        start_time = datetime.now()

        try:
            with logfire.span("intelligent_stage2_fallback", url=url, stage1_error=stage1_error):
                # Analyze Stage 1 failure for intelligent configuration
                anti_bot_level = 1  # Default enhanced level
                use_cache = True    # Always enable cache in fallback

                if stage1_error:
                    error_lower = stage1_error.lower()
                    if any(term in error_lower for term in ['blocked', 'forbidden', '403']):
                        anti_bot_level = 3  # Maximum stealth for blocking
                    elif any(term in error_lower for term in ['timeout', 'slow']):
                        anti_bot_level = 2  # Advanced configuration for timeouts
                    elif any(term in error_lower for term in ['dns', 'network']):
                        anti_bot_level = 0  # Basic configuration for DNS issues

                logger.info(f"üîÑ Stage 2 fallback for {url}: anti_bot_level={anti_bot_level}, cache_enabled={use_cache}")

                # Progressive anti-bot configuration
                if anti_bot_level >= 3:
                    # Level 3: Maximum stealth
                    config = CrawlerRunConfig(
                        cache_mode=CacheMode.ENABLED,
                        simulate_user=True,
                        magic=True,
                        wait_until="domcontentloaded",
                        page_timeout=60000,
                        delay_before_return_html=2.0,
                        headers={
                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                        }
                    )
                elif anti_bot_level >= 2:
                    # Level 2: Advanced
                    config = CrawlerRunConfig(
                        cache_mode=CacheMode.ENABLED,
                        simulate_user=True,
                        magic=True,
                        wait_until="domcontentloaded",
                        page_timeout=45000,
                        delay_before_return_html=1.0
                    )
                elif anti_bot_level >= 1:
                    # Level 1: Enhanced
                    config = CrawlerRunConfig(
                        cache_mode=CacheMode.ENABLED,
                        simulate_user=True,
                        magic=True,
                        wait_for="body",
                        page_timeout=30000
                    )
                else:
                    # Level 0: Basic (for DNS issues)
                    config = CrawlerRunConfig(
                        cache_mode=CacheMode.ENABLED,
                        wait_for="body",
                        page_timeout=30000
                    )

                # Use appropriate browser config
                browser_config = None
                if anti_bot_level >= 3 and 'stealth_browser_config' in self.browser_configs:
                    browser_config = self.browser_configs['stealth_browser_config']
                elif anti_bot_level >= 2 and 'base_browser_config' in self.browser_configs:
                    browser_config = self.browser_configs['base_browser_config']

                # Execute Stage 2 crawl
                if browser_config:
                    async with AsyncWebCrawler(config=browser_config) as crawler:
                        result = await crawler.arun(url=url, config=config)
                else:
                    async with AsyncWebCrawler() as crawler:
                        result = await crawler.arun(url=url, config=config)

                # Calculate metrics
                duration = (datetime.now() - start_time).total_seconds()
                content = result.markdown if result.success else None
                word_count = len(content.split()) if content else 0
                char_count = len(content) if content else 0

                # Update statistics
                self._stats['total_crawls'] += 1
                self._stats['stage2_fallbacks'] += 1
                if result.success:
                    self._stats['successful_crawls'] += 1
                else:
                    self._stats['failed_crawls'] += 1
                self._stats['total_duration'] += duration

                return OptimizedCrawlResult(
                    url=url,
                    success=result.success,
                    content=content,
                    error=result.error_message if not result.success else None,
                    duration=duration,
                    word_count=word_count,
                    char_count=char_count,
                    stage_used="2_intelligent",
                    cache_mode="enabled",
                    anti_bot_level=anti_bot_level,
                    dns_resolved=result.success
                )

        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds()
            self._stats['total_crawls'] += 1
            self._stats['failed_crawls'] += 1
            self._stats['total_duration'] += duration

            logger.error(f"‚ùå Intelligent Stage 2 fallback failed for {url}: {str(e)}")
            return OptimizedCrawlResult(
                url=url,
                success=False,
                error=str(e),
                duration=duration,
                stage_used="2_intelligent",
                cache_mode="error",
                anti_bot_level=anti_bot_level if 'anti_bot_level' in locals() else 0,
                dns_resolved=False
            )

    async def crawl_with_intelligent_fallback(self, url: str) -> OptimizedCrawlResult:
        """
        Execute crawling with intelligent Stage 1 ‚Üí Stage 2 fallback.

        Tries optimized Stage 1 first, then intelligently escalates to Stage 2
        based on the specific failure pattern.
        """
        logger.info(f"üöÄ Starting intelligent crawl for {url}")

        # Stage 1: Optimized fast crawl
        stage1_result = await self.optimized_stage1_crawl(url)

        if stage1_result.success:
            logger.info(f"‚úÖ Stage 1 successful for {url} ({stage1_result.char_count} chars in {stage1_result.duration:.2f}s)")
            return stage1_result

        logger.warning(f"‚ö†Ô∏è Stage 1 failed for {url}: {stage1_result.error}")

        # Stage 2: Intelligent fallback
        stage2_result = await self.intelligent_stage2_fallback(url, stage1_result.error)

        if stage2_result.success:
            logger.info(f"‚úÖ Stage 2 successful for {url} ({stage2_result.char_count} chars in {stage2_result.duration:.2f}s)")
        else:
            logger.error(f"‚ùå Both stages failed for {url}: {stage2_result.error}")

        return stage2_result

    async def crawl_multiple_optimized(
        self,
        urls: List[str],
        max_concurrent: int = 5
    ) -> List[OptimizedCrawlResult]:
        """
        Optimized parallel crawling with intelligent fallback for each URL.

        Maintains high throughput while ensuring reliability through intelligent fallback.
        """
        if not urls:
            return []

        with logfire.span("crawl_multiple_optimized", url_count=len(urls), max_concurrent=max_concurrent):
            # Create semaphore to limit concurrent operations
            semaphore = asyncio.Semaphore(max_concurrent)

            async def crawl_with_semaphore(url: str) -> OptimizedCrawlResult:
                async with semaphore:
                    return await self.crawl_with_intelligent_fallback(url)

            # Execute crawls concurrently
            tasks = [crawl_with_semaphore(url) for url in urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Handle any exceptions
            final_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    final_results.append(OptimizedCrawlResult(
                        url=urls[i],
                        success=False,
                        error=str(result),
                        stage_used="error"
                    ))
                else:
                    final_results.append(result)

            # Log summary
            successful = sum(1 for r in final_results if r.success)
            stage1_successful = sum(1 for r in final_results if r.stage_used == "1_optimized" and r.success)
            stage2_successful = sum(1 for r in final_results if r.stage_used == "2_intelligent" and r.success)

            logfire.info(
                "Optimized crawl batch completed",
                total_urls=len(urls),
                successful=successful,
                stage1_successful=stage1_successful,
                stage2_successful=stage2_successful,
                success_rate=successful / len(urls) if urls else 0,
                avg_duration=sum(r.duration for r in final_results) / len(final_results)
            )

            logger.info(f"üìä Optimized crawling completed: {successful}/{len(urls)} successful")
            logger.info(f"üéØ Stage 1 successes: {stage1_successful}, Stage 2 successes: {stage2_successful}")

            return final_results

    def get_performance_stats(self) -> Dict[str, Any]:
        """Get comprehensive performance statistics."""
        stats = self._stats.copy()
        if stats['total_crawls'] > 0:
            stats.update({
                'success_rate': stats['successful_crawls'] / stats['total_crawls'],
                'stage1_success_rate': stats['stage1_successes'] / stats['total_crawls'],
                'stage2_fallback_rate': stats['stage2_fallbacks'] / stats['total_crawls'],
                'dns_failure_rate': stats['dns_failures'] / stats['total_crawls'],
                'avg_duration': stats['total_duration'] / stats['total_crawls']
            })
        return stats


# Global optimized crawler instance
_global_optimized_crawler: Optional[OptimizedCrawler] = None


def get_optimized_crawler(browser_configs: Optional[Dict] = None) -> OptimizedCrawler:
    """Get or create global optimized crawler instance."""
    global _global_optimized_crawler
    if _global_optimized_crawler is None or browser_configs:
        _global_optimized_crawler = OptimizedCrawler(browser_configs)
    return _global_optimized_crawler


# Backward compatibility functions

async def optimized_scrape_and_clean_single_url(
    url: str,
    session_id: str = "default",
    search_query: str = None,
    extraction_mode: str = "article",
    include_metadata: bool = False,
    preserve_technical_content: bool = True
) -> dict:
    """
    Optimized single URL scraping with intelligent fallback and content cleaning.

    This function replaces the original scrape_and_clean_single_url_direct
    with the fixed Stage 1 configuration and intelligent fallback.
    """
    try:
        logger.info(f"üöÄ Starting optimized single URL scrape: {url}")

        total_start_time = datetime.now()

        # Get optimized crawler
        crawler = get_optimized_crawler()

        # Execute intelligent crawling
        crawl_result = await crawler.crawl_with_intelligent_fallback(url)

        if not crawl_result.success:
            return {
                'success': False,
                'url': url,
                'content': '',
                'cleaned_content': '',
                'error_message': crawl_result.error,
                'duration': crawl_result.duration,
                'word_count': 0,
                'char_count': 0,
                'extraction_mode': extraction_mode,
                'session_id': session_id,
                'stage': 'failed',
                'metadata': {
                    'stage_used': crawl_result.stage_used,
                    'cache_mode': crawl_result.cache_mode,
                    'anti_bot_level': crawl_result.anti_bot_level,
                    'dns_resolved': crawl_result.dns_resolved
                } if include_metadata else None
            }

        # Content cleaning (reuse existing logic)
        cleaned_content = crawl_result.content or ''

        try:
            # Import content cleaning utilities
            from utils.content_cleaning import clean_content_with_judge_optimization, assess_content_cleanliness

            if preserve_technical_content:
                # For technical content, use judge optimization
                logger.info(f"üß† Judge: Assessing content cleanliness for {url}")
                is_clean, judge_score = await assess_content_cleanliness(cleaned_content, url, 0.75)

                if is_clean:
                    logger.info(f"‚úÖ Content clean enough - skipping AI cleaning (saving ~35-40 seconds)")
                    cleaning_metadata = {
                        "judge_score": judge_score,
                        "cleaning_performed": False,
                        "optimization_used": True,
                        "latency_saved": "~35-40 seconds"
                    }
                else:
                    logger.info(f"üßΩ Content needs cleaning - running AI cleaning")
                    from utils.content_cleaning import clean_technical_content_with_gpt5_nano
                    cleaned_content = await clean_technical_content_with_gpt5_nano(
                        cleaned_content, url, search_query, session_id
                    )
                    cleaning_metadata = {
                        "judge_score": judge_score,
                        "cleaning_performed": True,
                        "optimization_used": True
                    }
            else:
                # Standard content cleaning
                cleaned_content, cleaning_metadata = await clean_content_with_judge_optimization(
                    cleaned_content, url, search_query, cleanliness_threshold=0.7
                )

        except Exception as clean_error:
            logger.warning(f"Content cleaning failed: {clean_error}, using original content")
            cleaning_metadata = {"cleaning_performed": False, "error": str(clean_error)}

        # Calculate final metrics
        total_duration = (datetime.now() - total_start_time).total_seconds()
        word_count = len(cleaned_content.split()) if cleaned_content else 0
        char_count = len(cleaned_content) if cleaned_content else 0

        logger.info(f"üéâ Optimized scraping completed: {url} ({char_count} chars in {total_duration:.2f}s)")

        return {
            'success': True,
            'url': url,
            'content': crawl_result.content or '',
            'cleaned_content': cleaned_content,
            'error_message': None,
            'duration': total_duration,
            'word_count': word_count,
            'char_count': char_count,
            'extraction_mode': extraction_mode,
            'session_id': session_id,
            'stage': crawl_result.stage_used,
            'metadata': {
                'title': '',  # Would be extracted from result if available
                'description': '',
                'status_code': None,
                'extraction_mode': extraction_mode,
                'preserve_technical': preserve_technical_content,
                'stage_duration': crawl_result.duration,
                'cleaning_optimization': cleaning_metadata,
                'stage_used': crawl_result.stage_used,
                'cache_mode': crawl_result.cache_mode,
                'anti_bot_level': crawl_result.anti_bot_level,
                'dns_resolved': crawl_result.dns_resolved,
                'optimized_crawler': True
            } if include_metadata else None
        }

    except Exception as e:
        logger.error(f"‚ùå Optimized scraping error for {url}: {e}")
        return {
            'success': False,
            'url': url,
            'content': '',
            'cleaned_content': '',
            'error_message': f'Optimized scraping error: {e}',
            'duration': 0,
            'word_count': 0,
            'char_count': 0,
            'extraction_mode': extraction_mode,
            'session_id': session_id,
            'stage': 'error'
        }


async def optimized_crawl_multiple_urls_with_cleaning(
    urls: List[str],
    session_id: str,
    search_query: str,
    max_concurrent: int = 10,
    extraction_mode: str = "article",
    include_metadata: bool = True
) -> List[dict]:
    """
    Optimized parallel crawling with immediate AI cleaning for each URL.

    This function provides the same interface as crawl_multiple_urls_with_cleaning
    but uses the optimized crawler with fixed Stage 1 configuration.
    """
    import asyncio
    from utils.content_cleaning import clean_content_with_gpt5_nano

    with logfire.span("optimized_crawl_multiple_urls_with_cleaning",
                     session_id=session_id,
                     url_count=len(urls),
                     search_query=search_query,
                     extraction_mode=extraction_mode,
                     max_concurrent=max_concurrent):

        logger.info(f"üöÄ Starting optimized parallel crawl+clean for {len(urls)} URLs")

        # Get optimized crawler
        crawler = get_optimized_crawler()

        async def optimized_crawl_and_clean_single(url: str) -> dict:
            """Crawl and clean a single URL with optimized configuration."""
            try:
                # Step 1: Optimized crawl with intelligent fallback
                crawl_results = await crawler.crawl_multiple(
                    urls=[url],
                    max_concurrent=1
                )

                if not crawl_results or not crawl_results[0].success:
                    return {
                        'url': url,
                        'success': False,
                        'content': '',
                        'cleaned_content': '',
                        'error_message': crawl_results[0].error if crawl_results else 'Crawl failed',
                        'duration': crawl_results[0].duration if crawl_results else 0,
                        'word_count': 0,
                        'char_count': 0,
                        'extraction_mode': extraction_mode,
                        'session_id': session_id
                    }

                crawl_result = crawl_results[0]

                # Step 2: Content cleaning if substantial
                cleaned_content = crawl_result.content or ''
                if len(cleaned_content.strip()) > 500:
                    try:
                        cleaned_content = await clean_content_with_gpt5_nano(
                            content=crawl_result.content,
                            url=url,
                            search_query=search_query
                        )
                        logger.info(f"‚úÖ Content cleaned for {url}: {len(crawl_result.content)} -> {len(cleaned_content)} chars")
                    except Exception as clean_error:
                        logger.warning(f"‚ö†Ô∏è Content cleaning failed for {url}: {clean_error}")
                        cleaned_content = crawl_result.content

                # Return structured result
                result = {
                    'url': url,
                    'success': True,
                    'content': crawl_result.content or '',
                    'cleaned_content': cleaned_content,
                    'error_message': None,
                    'duration': crawl_result.duration,
                    'word_count': crawl_result.word_count,
                    'char_count': len(cleaned_content),
                    'extraction_mode': extraction_mode,
                    'session_id': session_id
                }

                # Add metadata
                if include_metadata:
                    result.update({
                        'crawl_timestamp': datetime.now().isoformat(),
                        'search_query': search_query,
                        'content_cleaned': len(cleaned_content.strip()) > 200,
                        'stage_used': crawl_result.stage_used,
                        'cache_mode': crawl_result.cache_mode,
                        'anti_bot_level': crawl_result.anti_bot_level,
                        'dns_resolved': crawl_result.dns_resolved,
                        'optimized_crawler': True
                    })

                return result

            except Exception as e:
                logger.error(f"‚ùå Error in optimized crawl_and_clean for {url}: {e}")
                return {
                    'url': url,
                    'success': False,
                    'content': '',
                    'cleaned_content': '',
                    'error_message': str(e),
                    'duration': 0,
                    'word_count': 0,
                    'char_count': 0,
                    'extraction_mode': extraction_mode,
                    'session_id': session_id
                }

        # Execute all optimized crawl+clean operations concurrently
        tasks = [optimized_crawl_and_clean_single(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Handle exceptions
        final_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"‚ùå Task failed for URL {urls[i]}: {result}")
                final_results.append({
                    'url': urls[i],
                    'success': False,
                    'content': '',
                    'cleaned_content': '',
                    'error_message': str(result),
                    'duration': 0,
                    'word_count': 0,
                    'char_count': 0,
                    'extraction_mode': extraction_mode,
                    'session_id': session_id
                })
            else:
                final_results.append(result)

        successful_crawls = sum(1 for r in final_results if r['success'])
        logger.info(f"‚úÖ Optimized parallel crawl+clean completed: {successful_crawls}/{len(urls)} successful")

        return final_results


# Export optimized functions
__all__ = [
    'OptimizedCrawler',
    'OptimizedCrawlResult',
    'get_optimized_crawler',
    'optimized_scrape_and_clean_single_url',
    'optimized_crawl_multiple_urls_with_cleaning'
]