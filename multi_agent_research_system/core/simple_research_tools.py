"""Simplified research tools that coordinate with built-in Claude tools.

This module provides minimal coordination tools while letting agents use
the built-in WebSearch and WebFetch capabilities directly.
"""

import json
import os

# Import from parent directory structure
import sys
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("Warning: python-dotenv not found. Using environment variables only.")

# Import logging when available
try:
    from .logging_config import get_logger
    _logger = get_logger("research_tools")
    _logger.debug("Research tools logging initialized")
except ImportError:
    _logger = None

try:
    from claude_agent_sdk import tool
except ImportError:
    # Fallback decorator for when the SDK is not available
    def tool(name, description, parameters):
        def decorator(func):
            return func
        return decorator
    print("Warning: claude_agent_sdk not found. Using fallback tool decorator.")


@tool("save_research_findings", "Save research findings to session storage", {
    "topic": str,
    "findings": str,
    "sources": str,
    "session_id": str
})
async def save_research_findings(args: dict[str, Any]) -> dict[str, Any]:
    """Save research findings by returning data for external file creation."""
    if _logger:
        _logger.info(f"Preparing research findings for topic: {args.get('topic', 'Unknown')}")

    # Validate required arguments
    try:
        topic = args["topic"]
        findings = args["findings"]
        sources = args["sources"]
        session_id = args.get("session_id", str(uuid.uuid4()))
    except KeyError as e:
        error_msg = f"Missing required argument: {e}"
        return {"error": error_msg, "success": False}

    # Prepare research data structure
    research_data = {
        "topic": topic,
        "findings": findings,
        "sources": sources,
        "saved_at": datetime.now().isoformat(),
        "session_id": session_id
    }

    # Prepare file paths for external creation
    session_path = f"researchmaterials/sessions/{session_id}"
    findings_file = f"{session_path}/research_findings.json"

    # Use environment-aware path detection for KEVIN directory
    current_repo = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    if "claudeagent-multiagent-latest" in current_repo:
        kevin_dir = f"{current_repo}/KEVIN"
    else:
        kevin_dir = "/home/kjdragan/lrepos/claudeagent-multiagent-latest/KEVIN"
    timestamp = datetime.now().strftime('%H%M%S')
    kevin_findings_file = f"{kevin_dir}/research_findings_{session_id[:8]}_{timestamp}.json"

    if _logger:
        _logger.info(f"Research findings prepared for {topic} ({len(findings)} characters)")

    # Return data and file paths for external file creation
    return {
        "content": [{
            "type": "text",
            "text": f"Research findings for '{topic}' prepared for file creation. Session {session_id}, {len(findings)} characters of research content."
        }],
        "research_data": research_data,
        "session_file_path": findings_file,
        "kevin_file_path": kevin_findings_file,
        "session_id": session_id,
        "success": True
    }


@tool("create_research_report", "Create formatted report content and provide the filepath where it should be saved", {
    "topic": str,
    "content": str,
    "session_id": str,
    "format": str,
    "report_type": str  # e.g., "draft", "final", "editorial_review", "editorial_feedback"
})
async def create_research_report(args: dict[str, Any]) -> dict[str, Any]:
    """Create formatted report content and return it with the recommended filepath.

    NOTE: MCP tools cannot save files directly due to sandboxing. This tool formats the report
    and returns the content along with the recommended filepath. The agent MUST then use the
    Write tool to save the content to the recommended filepath.
    """
    if _logger:
        _logger.info(f"Creating research report for topic: {args.get('topic', 'Unknown')}")

    topic = args["topic"]
    content = args["content"]
    session_id = args.get("session_id", str(uuid.uuid4()))
    format_type = args.get("format", "markdown")
    report_type = args.get("report_type", "report")  # draft, final, editorial_review, etc.

    # Create formatted report content
    report_content = f"""# Research Report: {topic}

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Session ID:** {session_id}
**Report Type:** {report_type}

---

{content}

---

*This report was generated by the Multi-Agent Research System using Claude Agent SDK.*
"""

    # Determine the correct subdirectory based on report type
    if "editorial" in report_type.lower() or "review" in report_type.lower() or "feedback" in report_type.lower():
        subdir = "working"  # Editorial reviews and feedback go to working directory
    elif report_type.lower() == "final":
        subdir = "working"  # Final reports go to working directory
    else:
        subdir = "working"  # Drafts and other reports go to working directory

    # Generate timestamp in new format
    timestamp = datetime.now().strftime('%m-%d_%H:%M:%S')
    # Always use .md extension for all reports (markdown is the standard format)
    extension = "md"

    # Simplified file naming with prefixes and no titles in filenames
    # Format: PREFIX_NAME_timestamp.ext or PREFIX_revision_number_timestamp.ext
    # NOTE: Research scope (brief/default/comprehensive) affects research parameters, NOT filename
    prefixes = {
        "draft": "EXECUTIVE_SUMMARY_DRAFT",           # Brief overview document
        "comprehensive_report": "INITIAL_REPORT_DRAFT",  # Main report (regardless of research scope)
        "comprehensive_analysis": "INITIAL_REPORT_DRAFT",  # Same as above - main report
        "brief": "INITIAL_REPORT_DRAFT",              # Main report (research scope doesn't affect name)
        "default": "INITIAL_REPORT_DRAFT",            # Main report (research scope doesn't affect name)
        "editorial_review": "EDITORIAL_RECOMMENDATIONS",
        "editorial_feedback": "EDITORIAL_FEEDBACK",
        "research_findings": "RESEARCH",
        "search_verification": "SEARCH_VERIFY",
        "revise": "REVISED",
        "revision": "REVISED"
    }
    
    prefix = prefixes.get(report_type.lower(), report_type.upper())
    
    # Special naming for editorial reviews and revisions
    if report_type.lower() == "editorial_review":
        filename = f"{prefix}_{timestamp}.{extension}"
    elif report_type.lower() in ["revise", "revision"]:
        # For revision files, use REVISED_Edited #[number] format
        # Extract revision number from content or use default #1
        revision_number = "1"  # Default to first revision
        filename = f"REVISED_Edited #{revision_number}_{timestamp}.{extension}"
    else:
        filename = f"{prefix}_{timestamp}.{extension}"
    
    # Use absolute path to ensure agents save files to correct location
    from pathlib import Path
    recommended_filepath = str(Path.cwd() / f"KEVIN/sessions/{session_id}/{subdir}/{filename}")

    if _logger:
        _logger.info(f"Research report formatted for {topic} ({len(report_content)} characters)")

    # Return instructions for the agent to save the file
    instruction_text = f"""Report content created for '{topic}' ({len(report_content)} characters).

⚠️  CRITICAL: You MUST now save this report using the Write tool.

Use this EXACT filepath:
    {recommended_filepath}

The Write tool will automatically create any necessary directories."""

    return {
        "content": [{
            "type": "text",
            "text": instruction_text
        }],
        "report_content": report_content,
        "recommended_filepath": recommended_filepath,
        "session_id": session_id,
        "report_size": len(report_content),
        "report_type": report_type,
        "success": True
    }


@tool("get_session_data", "Retrieve data from a research session", {
    "session_id": str,
    "data_type": str
})
async def get_session_data(args: dict[str, Any]) -> dict[str, Any]:
    """Retrieve data from a research session."""
    session_id = args["session_id"]
    data_type = args.get("data_type", "all")

    # Try KEVIN directory structure first (primary)
    # Use environment-aware path detection for KEVIN base directory
    current_repo = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    if "claudeagent-multiagent-latest" in current_repo:
        kevin_default = f"{current_repo}/KEVIN"
    else:
        kevin_default = "/home/kjdragan/lrepos/claudeagent-multiagent-latest/KEVIN"
    kevin_base = os.environ.get('KEVIN_WORKPRODUCTS_DIR', kevin_default)
    session_path = Path(f"{kevin_base}/sessions/{session_id}")

    # Fallback to legacy researchmaterials structure
    if not session_path.exists():
        session_path = Path(f"researchmaterials/sessions/{session_id}")

    if not session_path.exists():
        return {
            "content": [{
                "type": "text",
                "text": f"Session {session_id} not found in KEVIN or researchmaterials directories."
            }]
        }

    result_data = {}

    # Try to load research findings (new standardized format)
    if data_type in ["all", "findings"]:
        findings_file = session_path / "research_findings.json"
        if findings_file.exists():
            try:
                with open(findings_file, encoding='utf-8') as f:
                    findings_data = json.load(f)
                    result_data["findings"] = findings_data

                    # For backward compatibility, also provide structured data for report agents
                    if "sources" in findings_data and "findings" in findings_data:
                        # This is the new standardized format
                        result_data["standardized_research"] = {
                            "research_topic": findings_data.get("research_topic"),
                            "research_timestamp": findings_data.get("research_timestamp"),
                            "sources": findings_data.get("sources", []),
                            "findings": findings_data.get("findings", []),
                            "key_themes": findings_data.get("key_themes", []),
                            "content_summary": findings_data.get("content_summary", ""),
                            "search_metrics": findings_data.get("search_metrics", []),
                            "source_analysis": findings_data.get("source_analysis", {}),
                            "quality_assessment": findings_data.get("quality_assessment", {}),
                            "research_metadata": findings_data.get("research_metadata", {})
                        }
            except (json.JSONDecodeError, Exception) as e:
                # Handle corrupted research findings file
                result_data["findings_error"] = f"Error reading research findings: {str(e)}"
        else:
            # Fallback: look for any research work products in the session directory
            research_dir = session_path / "research"
            if research_dir.exists():
                research_files = list(research_dir.glob("*.md"))
                if research_files:
                    result_data["fallback_research_files"] = [str(f) for f in research_files]
                    # Try to read the most recent research file
                    try:
                        latest_file = max(research_files, key=os.path.getctime)
                        with open(latest_file, encoding='utf-8') as f:
                            result_data["fallback_research_content"] = f.read()
                    except Exception as e:
                        result_data["fallback_error"] = f"Error reading fallback research file: {str(e)}"

    # Try to load research report
    if data_type in ["all", "report"]:
        report_files = list(session_path.glob("research_report.*"))
        if report_files:
            try:
                report_file = report_files[0]
                with open(report_file, encoding='utf-8') as f:
                    result_data["report"] = f.read()
            except Exception as e:
                result_data["report_error"] = f"Error reading research report: {str(e)}"
        else:
            # Fallback: look for any markdown files that might be reports in subdirectories
            for subdir in ["research", "working", "final"]:
                subdir_path = session_path / subdir
                if subdir_path.exists():
                    # Match various report types including new naming convention
                    report_files = list(subdir_path.glob("*.md"))
                    # Filter for report-like files (supports both old and new naming)
                    report_files = [f for f in report_files if any(keyword in f.name.upper() 
                        for keyword in ["REPORT", "DRAFT", "SUMMARY", "EDITORIAL", "REVISED", "COMPREHENSIVE", "BRIEF"])]
                    if report_files:
                        try:
                            latest_report = max(report_files, key=os.path.getctime)
                            with open(latest_report, encoding='utf-8') as f:
                                result_data["fallback_report"] = f.read()
                                result_data["fallback_report_path"] = str(latest_report)
                                break
                        except Exception as e:
                            result_data["fallback_report_error"] = f"Error reading fallback report: {str(e)}"

    # Try to load session state
    if data_type in ["all", "state"]:
        state_file = session_path / "session_state.json"
        if state_file.exists():
            with open(state_file, encoding='utf-8') as f:
                result_data["state"] = json.load(f)

    # Create comprehensive status message
    status_items = []
    if "findings" in result_data:
        status_items.append("research findings")
    if "standardized_research" in result_data:
        status_items.append("structured research data")
    if "fallback_research_content" in result_data:
        status_items.append("fallback research content")
    if "report" in result_data:
        status_items.append("research report")
    if "fallback_report" in result_data:
        status_items.append("fallback report")
    if "state" in result_data:
        status_items.append("session state")

    # Add error messages if any
    error_messages = []
    if "findings_error" in result_data:
        error_messages.append(result_data["findings_error"])
    if "report_error" in result_data:
        error_messages.append(result_data["report_error"])
    if "fallback_error" in result_data:
        error_messages.append(result_data["fallback_error"])

    if status_items:
        status_text = f"Successfully retrieved {', '.join(status_items)} for session {session_id}"
    else:
        status_text = f"No data found for session {session_id}"

    if error_messages:
        status_text += "\n\nWarnings:\n" + "\n".join(error_messages)

    return {
        "content": [{
            "type": "text",
            "text": status_text
        }],
        "session_data": result_data,
        "data_availability": {
            "has_research_findings": "findings" in result_data,
            "has_structured_research": "standardized_research" in result_data,
            "has_fallback_research": "fallback_research_content" in result_data,
            "has_report": "report" in result_data,
            "has_fallback_report": "fallback_report" in result_data,
            "has_session_state": "state" in result_data,
            "data_types_found": list(result_data.keys())
        }
    }


@tool("request_gap_research", "Request orchestrator to execute gap-filling research for identified information gaps", {
    "gaps": list[str],
    "session_id": str,
    "priority": str,
    "context": str
})
async def request_gap_research(args: dict[str, Any]) -> dict[str, Any]:
    """
    Request orchestrator to execute gap-filling research.

    This tool creates a structured request that the orchestrator will detect and handle
    by coordinating the research agent to execute targeted gap-filling searches using
    the proven successful workflow.

    Args:
        gaps: List of specific information gaps/topics to research (e.g.,
              ["Russia Ukraine October 2025 casualties", "frontline changes"])
        session_id: Current research session ID
        priority: Priority level - "high", "medium", or "low"
        context: Additional context about why these gaps need to be filled

    Returns:
        Structured request that orchestrator will process
    """
    gaps = args["gaps"]
    session_id = args["session_id"]
    priority = args.get("priority", "medium")
    context = args.get("context", "Editorial review identified information gaps")

    # Validate inputs
    if not gaps or len(gaps) == 0:
        return {
            "content": [{
                "type": "text",
                "text": "⚠️ No research gaps provided. Please specify at least one gap to research."
            }],
            "is_error": True
        }

    if len(gaps) > 3:
        gaps = gaps[:3]  # Limit to top 3 gaps for focused research
        if _logger:
            _logger.info(f"Limited gap research to top 3 gaps (from {len(args['gaps'])} provided)")

    # Create structured request
    request = {
        "type": "editorial_gap_research_request",
        "session_id": session_id,
        "gaps": gaps,
        "gap_count": len(gaps),
        "priority": priority,
        "context": context,
        "timestamp": datetime.now().isoformat(),
        "requested_by": "editor_agent"
    }

    if _logger:
        _logger.info(f"Gap research request created: {len(gaps)} gaps for session {session_id[:8]}")

    return {
        "content": [{
            "type": "text",
            "text": f"""✅ Gap research request created successfully.

**Gaps to Research:** {len(gaps)}
{chr(10).join([f'  {i+1}. {gap}' for i, gap in enumerate(gaps)])}

**Priority:** {priority}
**Context:** {context}

The orchestrator will execute this research using the proven research workflow and return results for integration into your editorial review."""
        }],
        "gap_research_request": request,
        "success": True
    }
