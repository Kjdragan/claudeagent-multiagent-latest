"""
Analytics Engine for automated reporting and performance analysis.

This module provides comprehensive analytics capabilities for log data,
including trend analysis, anomaly detection, and automated insights.
"""

import statistics
from collections import Counter, defaultdict
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

from .log_aggregator import LogEntry
from .log_search import LogSearchEngine


@dataclass
class AnalyticsMetric:
    """Analytics metric data structure."""
    name: str
    value: float
    unit: str
    timestamp: datetime
    metadata: dict[str, Any]
    trend: str | None = None  # 'up', 'down', 'stable'
    change_percent: float | None = None


@dataclass
class TrendAnalysis:
    """Trend analysis results."""
    metric_name: str
    time_period: str
    current_value: float
    previous_value: float
    change_percent: float
    trend_direction: str  # 'increasing', 'decreasing', 'stable'
    confidence: float  # 0-1
    data_points: list[tuple[datetime, float]]


@dataclass
class AnomalyDetection:
    """Anomaly detection results."""
    metric_name: str
    timestamp: datetime
    actual_value: float
    expected_value: float
    deviation_percent: float
    severity: str  # 'low', 'medium', 'high', 'critical'
    description: str


@dataclass
class PerformanceInsight:
    """Performance insight generated by analytics."""
    category: str  # 'performance', 'usage', 'errors', 'efficiency'
    title: str
    description: str
    impact_level: str  # 'low', 'medium', 'high'
    recommendations: list[str]
    supporting_data: dict[str, Any]
    timestamp: datetime


class AnalyticsEngine:
    """Advanced analytics engine for log data analysis."""

    def __init__(self,
                 session_id: str,
                 analytics_dir: str = "analytics"):
        """
        Initialize the analytics engine.

        Args:
            session_id: Session identifier
            analytics_dir: Directory to store analytics data
        """
        self.session_id = session_id
        self.analytics_dir = Path(analytics_dir)
        self.analytics_dir.mkdir(parents=True, exist_ok=True)

        # Initialize search engine
        self.search_engine = LogSearchEngine()

        # Analytics data storage
        self.metrics_history: dict[str, list[AnalyticsMetric]] = defaultdict(list)
        self.trends: dict[str, TrendAnalysis] = {}
        self.anomalies: list[AnomalyDetection] = []
        self.insights: list[PerformanceInsight] = []

        # Analytics configuration
        self.trend_thresholds = {
            'significant_change': 10.0,  # Percent change considered significant
            'trend_confidence': 0.7,     # Minimum confidence for trend detection
            'min_data_points': 5         # Minimum data points for trend analysis
        }

        self.anomaly_thresholds = {
            'deviation_percent': 25.0,    # Percent deviation for anomaly detection
            'min_data_points': 10,       # Minimum data points for baseline
            'severity_levels': {
                'low': 25.0,
                'medium': 50.0,
                'high': 75.0,
                'critical': 100.0
            }
        }

    async def analyze_logs(self,
                          entries: list[LogEntry],
                          analysis_types: list[str] | None = None) -> dict[str, Any]:
        """
        Perform comprehensive log analysis.

        Args:
            entries: List of log entries to analyze
            analysis_types: Types of analysis to perform (default: all)

        Returns:
            Analysis results
        """
        if not entries:
            return {'error': 'No log entries provided for analysis'}

        if analysis_types is None:
            analysis_types = ['performance', 'usage', 'errors', 'trends', 'anomalies']

        # Build search index for efficient analysis
        self.search_engine.build_index(entries)

        results = {
            'session_id': self.session_id,
            'analysis_timestamp': datetime.now().isoformat(),
            'total_entries': len(entries),
            'time_range': {
                'start': min(entry.timestamp for entry in entries).isoformat(),
                'end': max(entry.timestamp for entry in entries).isoformat()
            },
            'analysis_results': {}
        }

        # Perform different types of analysis
        if 'performance' in analysis_types:
            results['analysis_results']['performance'] = await self._analyze_performance(entries)

        if 'usage' in analysis_types:
            results['analysis_results']['usage'] = await self._analyze_usage(entries)

        if 'errors' in analysis_types:
            results['analysis_results']['errors'] = await self._analyze_errors(entries)

        if 'trends' in analysis_types:
            results['analysis_results']['trends'] = await self._analyze_trends(entries)

        if 'anomalies' in analysis_types:
            results['analysis_results']['anomalies'] = await self._detect_anomalies(entries)

        # Generate insights
        results['insights'] = await self._generate_insights(results['analysis_results'])

        return results

    async def _analyze_performance(self, entries: list[LogEntry]) -> dict[str, Any]:
        """Analyze performance metrics from log entries."""
        performance_metrics = {}

        # Response time analysis
        response_times = []
        for entry in entries:
            if 'execution_time' in entry.metadata:
                response_times.append(entry.metadata['execution_time'])

        if response_times:
            performance_metrics['response_times'] = {
                'avg': statistics.mean(response_times),
                'median': statistics.median(response_times),
                'min': min(response_times),
                'max': max(response_times),
                'std_dev': statistics.stdev(response_times) if len(response_times) > 1 else 0,
                'percentiles': {
                    '50th': statistics.median(response_times),
                    '90th': self._percentile(response_times, 90),
                    '95th': self._percentile(response_times, 95),
                    '99th': self._percentile(response_times, 99)
                }
            }

        # Throughput analysis
        time_buckets = self._bucket_entries_by_time(entries, timedelta(minutes=5))
        throughput_rates = [len(bucket) for bucket in time_buckets.values()]

        if throughput_rates:
            performance_metrics['throughput'] = {
                'avg_requests_per_minute': statistics.mean(throughput_rates) * 12,  # Convert to per minute
                'peak_requests_per_minute': max(throughput_rates) * 12,
                'min_requests_per_minute': min(throughput_rates) * 12,
                'total_requests': len(entries)
            }

        # Error rate analysis
        error_entries = [e for e in entries if e.level in ['ERROR', 'CRITICAL']]
        performance_metrics['error_rate'] = {
            'total_errors': len(error_entries),
            'error_rate_percent': (len(error_entries) / len(entries)) * 100 if entries else 0,
            'errors_by_agent': Counter(e.agent_name for e in error_entries if e.agent_name),
            'errors_by_type': Counter(e.activity_type for e in error_entries if e.activity_type)
        }

        # Agent performance analysis
        agent_performance = defaultdict(list)
        for entry in entries:
            if entry.agent_name and 'execution_time' in entry.metadata:
                agent_performance[entry.agent_name].append(entry.metadata['execution_time'])

        performance_metrics['agent_performance'] = {}
        for agent, times in agent_performance.items():
            if times:
                performance_metrics['agent_performance'][agent] = {
                    'avg_execution_time': statistics.mean(times),
                    'total_executions': len(times),
                    'performance_trend': 'improving' if len(times) > 1 and times[-1] < times[0] else 'degrading'
                }

        return performance_metrics

    async def _analyze_usage(self, entries: list[LogEntry]) -> dict[str, Any]:
        """Analyze usage patterns and statistics."""
        usage_stats = {}

        # Activity frequency analysis
        activity_types = Counter(e.activity_type for e in entries if e.activity_type)
        usage_stats['activity_frequency'] = dict(activity_types.most_common(10))

        # Agent usage analysis
        agent_usage = Counter(e.agent_name for e in entries if e.agent_name)
        usage_stats['agent_usage'] = dict(agent_usage.most_common())

        # Time-based usage patterns
        hourly_usage = defaultdict(int)
        daily_usage = defaultdict(int)

        for entry in entries:
            hourly_usage[entry.timestamp.hour] += 1
            daily_usage[entry.timestamp.strftime('%Y-%m-%d')] += 1

        usage_stats['temporal_patterns'] = {
            'hourly_distribution': dict(hourly_usage),
            'daily_distribution': dict(daily_usage),
            'peak_hour': max(hourly_usage, key=hourly_usage.get) if hourly_usage else None,
            'peak_day': max(daily_usage, key=daily_usage.get) if daily_usage else None
        }

        # Session analysis
        session_counts = Counter(e.session_id for e in entries)
        usage_stats['session_stats'] = {
            'total_sessions': len(session_counts),
            'avg_entries_per_session': statistics.mean(list(session_counts.values())),
            'most_active_session': session_counts.most_common(1)[0] if session_counts else None
        }

        # Tool usage analysis
        tool_usage = Counter()
        for entry in entries:
            if 'tool_name' in entry.metadata:
                tool_usage[entry.metadata['tool_name']] += 1

        usage_stats['tool_usage'] = dict(tool_usage.most_common(10))

        return usage_stats

    async def _analyze_errors(self, entries: list[LogEntry]) -> dict[str, Any]:
        """Analyze error patterns and trends."""
        error_entries = [e for e in entries if e.level in ['ERROR', 'CRITICAL']]

        if not error_entries:
            return {'total_errors': 0, 'message': 'No errors found in the analyzed entries'}

        error_analysis = {
            'total_errors': len(error_entries),
            'error_rate_percent': (len(error_entries) / len(entries)) * 100,
            'errors_by_level': Counter(e.level for e in error_entries),
            'errors_by_agent': Counter(e.agent_name for e in error_entries if e.agent_name),
            'errors_by_activity': Counter(e.activity_type for e in error_entries if e.activity_type),
            'error_messages': Counter(e.message for e in error_entries)
        }

        # Error trend analysis
        error_timeline = self._bucket_entries_by_time(error_entries, timedelta(hours=1))
        error_trend = self._calculate_trend(list(error_timeline.values()))

        error_analysis['error_trend'] = {
            'direction': error_trend['direction'],
            'change_percent': error_trend['change_percent'],
            'hourly_distribution': {k.isoformat(): v for k, v in error_timeline.items()}
        }

        # Most common error patterns
        error_patterns = []
        for message, count in error_analysis['error_messages'].most_common(5):
            error_patterns.append({
                'message': message,
                'count': count,
                'percentage': (count / len(error_entries)) * 100
            })

        error_analysis['top_error_patterns'] = error_patterns

        return error_analysis

    async def _analyze_trends(self, entries: list[LogEntry]) -> dict[str, Any]:
        """Analyze trends in various metrics."""
        trends = {}

        # Create time series data
        time_buckets = self._bucket_entries_by_time(entries, timedelta(hours=1))

        # Activity volume trend
        activity_volumes = list(time_buckets.values())
        if len(activity_volumes) >= self.trend_thresholds['min_data_points']:
            volume_trend = self._calculate_trend(activity_volumes)
            trends['activity_volume'] = TrendAnalysis(
                metric_name='activity_volume',
                time_period='hourly',
                current_value=activity_volumes[-1] if activity_volumes else 0,
                previous_value=activity_volumes[0] if activity_volumes else 0,
                change_percent=volume_trend['change_percent'],
                trend_direction=volume_trend['direction'],
                confidence=volume_trend['confidence'],
                data_points=list(time_buckets.items())
            )

        # Error rate trend
        error_buckets = self._bucket_entries_by_time(
            [e for e in entries if e.level in ['ERROR', 'CRITICAL']],
            timedelta(hours=1)
        )

        error_rates = []
        for timestamp in time_buckets:
            total_count = len(time_buckets[timestamp])
            error_count = len(error_buckets.get(timestamp, []))
            error_rate = (error_count / total_count * 100) if total_count > 0 else 0
            error_rates.append(error_rate)

        if len(error_rates) >= self.trend_thresholds['min_data_points']:
            error_trend = self._calculate_trend(error_rates)
            trends['error_rate'] = TrendAnalysis(
                metric_name='error_rate',
                time_period='hourly',
                current_value=error_rates[-1] if error_rates else 0,
                previous_value=error_rates[0] if error_rates else 0,
                change_percent=error_trend['change_percent'],
                trend_direction=error_trend['direction'],
                confidence=error_trend['confidence'],
                data_points=list(time_buckets.items())
            )

        # Agent activity trends
        agent_trends = {}
        for agent_name in set(e.agent_name for e in entries if e.agent_name):
            agent_entries = [e for e in entries if e.agent_name == agent_name]
            agent_buckets = self._bucket_entries_by_time(agent_entries, timedelta(hours=1))
            agent_volumes = list(agent_buckets.values())

            if len(agent_volumes) >= self.trend_thresholds['min_data_points']:
                agent_trend = self._calculate_trend(agent_volumes)
                agent_trends[agent_name] = TrendAnalysis(
                    metric_name=f'{agent_name}_activity',
                    time_period='hourly',
                    current_value=agent_volumes[-1] if agent_volumes else 0,
                    previous_value=agent_volumes[0] if agent_volumes else 0,
                    change_percent=agent_trend['change_percent'],
                    trend_direction=agent_trend['direction'],
                    confidence=agent_trend['confidence'],
                    data_points=list(agent_buckets.items())
                )

        trends['agent_activity'] = agent_trends

        return trends

    async def _detect_anomalies(self, entries: list[LogEntry]) -> dict[str, Any]:
        """Detect anomalies in log data."""
        anomalies = []

        # Detect activity volume anomalies
        time_buckets = self._bucket_entries_by_time(entries, timedelta(minutes=15))
        volumes = list(time_buckets.values())

        if len(volumes) >= self.anomaly_thresholds['min_data_points']:
            volume_anomalies = self._detect_statistical_anomalies(
                volumes, list(time_buckets.keys()), 'activity_volume'
            )
            anomalies.extend(volume_anomalies)

        # Detect error rate anomalies
        error_buckets = self._bucket_entries_by_time(
            [e for e in entries if e.level in ['ERROR', 'CRITICAL']],
            timedelta(minutes=15)
        )

        error_rates = []
        for timestamp in time_buckets:
            total_count = len(time_buckets[timestamp])
            error_count = len(error_buckets.get(timestamp, []))
            error_rate = (error_count / total_count * 100) if total_count > 0 else 0
            error_rates.append(error_rate)

        if len(error_rates) >= self.anomaly_thresholds['min_data_points']:
            error_anomalies = self._detect_statistical_anomalies(
                error_rates, list(time_buckets.keys()), 'error_rate'
            )
            anomalies.extend(error_anomalies)

        # Detect response time anomalies
        response_times = []
        response_timestamps = []

        for entry in entries:
            if 'execution_time' in entry.metadata:
                response_times.append(entry.metadata['execution_time'])
                response_timestamps.append(entry.timestamp)

        if len(response_times) >= self.anomaly_thresholds['min_data_points']:
            time_anomalies = self._detect_statistical_anomalies(
                response_times, response_timestamps, 'response_time'
            )
            anomalies.extend(time_anomalies)

        self.anomalies.extend(anomalies)

        return {
            'total_anomalies': len(anomalies),
            'anomalies_by_severity': Counter(a.severity for a in anomalies),
            'anomalies_by_type': Counter(a.metric_name for a in anomalies),
            'recent_anomalies': [asdict(a) for a in sorted(anomalies, key=lambda x: x.timestamp, reverse=True)[:10]]
        }

    async def _generate_insights(self, analysis_results: dict[str, Any]) -> list[PerformanceInsight]:
        """Generate performance insights from analysis results."""
        insights = []

        # Performance insights
        if 'performance' in analysis_results:
            perf_data = analysis_results['performance']

            # Response time insights
            if 'response_times' in perf_data:
                avg_response = perf_data['response_times']['avg']
                if avg_response > 5.0:  # 5 seconds threshold
                    insights.append(PerformanceInsight(
                        category='performance',
                        title='High Response Times Detected',
                        description=f'Average response time of {avg_response:.2f}s exceeds recommended threshold.',
                        impact_level='high' if avg_response > 10.0 else 'medium',
                        recommendations=[
                            'Investigate slow operations and optimize bottlenecks',
                            'Consider implementing caching mechanisms',
                            'Review resource allocation and scaling needs'
                        ],
                        supporting_data={'avg_response_time': avg_response},
                        timestamp=datetime.now()
                    ))

            # Error rate insights
            if 'error_rate' in perf_data:
                error_rate = perf_data['error_rate']['error_rate_percent']
                if error_rate > 5.0:  # 5% error rate threshold
                    insights.append(PerformanceInsight(
                        category='errors',
                        title='High Error Rate Detected',
                        description=f'Error rate of {error_rate:.2f}% indicates system issues.',
                        impact_level='high' if error_rate > 10.0 else 'medium',
                        recommendations=[
                            'Review error logs for common patterns',
                            'Implement better error handling and retry mechanisms',
                            'Investigate root causes of frequent failures'
                        ],
                        supporting_data={'error_rate_percent': error_rate},
                        timestamp=datetime.now()
                    ))

        # Usage insights
        if 'usage' in analysis_results:
            usage_data = analysis_results['usage']

            # Peak usage insights
            if 'temporal_patterns' in usage_data:
                peak_hour = usage_data['temporal_patterns'].get('peak_hour')
                if peak_hour is not None:
                    insights.append(PerformanceInsight(
                        category='usage',
                        title=f'Peak Usage Activity at Hour {peak_hour}',
                        description=f'System experiences highest activity around {peak_hour}:00.',
                        impact_level='medium',
                        recommendations=[
                            'Ensure adequate resources during peak hours',
                            'Consider load balancing strategies',
                            'Schedule maintenance during off-peak hours'
                        ],
                        supporting_data={'peak_hour': peak_hour},
                        timestamp=datetime.now()
                    ))

        # Trend insights
        if 'trends' in analysis_results:
            trends_data = analysis_results['trends']

            for trend_name, trend in trends_data.items():
                if isinstance(trend, TrendAnalysis):
                    if (trend.change_percent > self.trend_thresholds['significant_change'] and
                        trend.confidence > self.trend_thresholds['trend_confidence']):

                        direction = "increasing" if trend.trend_direction == 'increasing' else "decreasing"
                        insights.append(PerformanceInsight(
                            category='performance',
                            title=f'Significant {direction.title()} Trend in {trend.metric_name}',
                            description=f'{trend.metric_name} has {direction} by {abs(trend.change_percent):.1f}%.',
                            impact_level='high' if abs(trend.change_percent) > 25.0 else 'medium',
                            recommendations=[
                                f'Monitor {trend.metric_name} closely',
                                'Investigate underlying causes of this trend',
                                'Take corrective action if trend is concerning'
                            ],
                            supporting_data={
                                'trend_direction': trend.trend_direction,
                                'change_percent': trend.change_percent,
                                'confidence': trend.confidence
                            },
                            timestamp=datetime.now()
                        ))

        # Anomaly insights
        if 'anomalies' in analysis_results:
            anomalies_data = analysis_results['anomalies']

            critical_anomalies = [a for a in self.anomalies if a.severity == 'critical']
            if critical_anomalies:
                insights.append(PerformanceInsight(
                    category='performance',
                    title='Critical Anomalies Detected',
                    description=f'{len(critical_anomalies)} critical anomalies require immediate attention.',
                    impact_level='high',
                    recommendations=[
                        'Investigate critical anomalies immediately',
                        'Review system health and stability',
                        'Implement proactive monitoring alerts'
                    ],
                    supporting_data={'critical_anomaly_count': len(critical_anomalies)},
                    timestamp=datetime.now()
                ))

        self.insights.extend(insights)
        return insights

    def _bucket_entries_by_time(self, entries: list[LogEntry], bucket_size: timedelta) -> dict[datetime, list[LogEntry]]:
        """Bucket entries by time intervals."""
        buckets = defaultdict(list)

        if not entries:
            return buckets

        start_time = min(entry.timestamp for entry in entries)
        end_time = max(entry.timestamp for entry in entries)

        current_time = start_time
        while current_time <= end_time:
            bucket_end = current_time + bucket_size
            for entry in entries:
                if current_time <= entry.timestamp < bucket_end:
                    buckets[current_time].append(entry)
            current_time = bucket_end

        return dict(buckets)

    def _percentile(self, data: list[float], percentile: int) -> float:
        """Calculate percentile of data."""
        if not data:
            return 0.0
        sorted_data = sorted(data)
        index = (percentile / 100) * (len(sorted_data) - 1)
        if index.is_integer():
            return sorted_data[int(index)]
        else:
            lower = sorted_data[int(index)]
            upper = sorted_data[int(index) + 1]
            return lower + (upper - lower) * (index - int(index))

    def _calculate_trend(self, values: list[float]) -> dict[str, Any]:
        """Calculate trend statistics."""
        if len(values) < 2:
            return {'direction': 'stable', 'change_percent': 0.0, 'confidence': 0.0}

        # Simple linear regression for trend
        n = len(values)
        x_values = list(range(n))

        # Calculate slope
        x_mean = statistics.mean(x_values)
        y_mean = statistics.mean(values)

        numerator = sum((x - x_mean) * (y - y_mean) for x, y in zip(x_values, values, strict=False))
        denominator = sum((x - x_mean) ** 2 for x in x_values)

        if denominator == 0:
            return {'direction': 'stable', 'change_percent': 0.0, 'confidence': 0.0}

        slope = numerator / denominator

        # Calculate trend direction
        if slope > 0.1:  # Threshold for meaningful trend
            direction = 'increasing'
        elif slope < -0.1:
            direction = 'decreasing'
        else:
            direction = 'stable'

        # Calculate change percentage
        if values[0] != 0:
            change_percent = ((values[-1] - values[0]) / values[0]) * 100
        else:
            change_percent = 0.0

        # Calculate confidence based on consistency
        if len(values) > 2:
            deviations = [abs(values[i] - values[i-1]) for i in range(1, len(values))]
            avg_deviation = statistics.mean(deviations)
            confidence = max(0, 1 - (avg_deviation / (statistics.mean(values) + 1e-6)))
        else:
            confidence = 0.5

        return {
            'direction': direction,
            'change_percent': change_percent,
            'confidence': confidence
        }

    def _detect_statistical_anomalies(self, values: list[float], timestamps: list[datetime], metric_name: str) -> list[AnomalyDetection]:
        """Detect statistical anomalies in time series data."""
        anomalies = []

        if len(values) < self.anomaly_thresholds['min_data_points']:
            return anomalies

        # Calculate baseline statistics
        mean_val = statistics.mean(values)
        std_dev = statistics.stdev(values) if len(values) > 1 else 0

        # Detect anomalies
        threshold = self.anomaly_thresholds['deviation_percent'] / 100.0
        for i, (value, timestamp) in enumerate(zip(values, timestamps, strict=False)):
            if std_dev > 0:
                z_score = abs(value - mean_val) / std_dev
                deviation_percent = (abs(value - mean_val) / mean_val) * 100 if mean_val != 0 else 0

                if deviation_percent > threshold:
                    # Determine severity
                    severity = 'low'
                    for level, thresh in self.anomaly_thresholds['severity_levels'].items():
                        if deviation_percent >= thresh:
                            severity = level

                    anomaly = AnomalyDetection(
                        metric_name=metric_name,
                        timestamp=timestamp,
                        actual_value=value,
                        expected_value=mean_val,
                        deviation_percent=deviation_percent,
                        severity=severity,
                        description=f'{metric_name} deviated by {deviation_percent:.1f}% from expected value'
                    )
                    anomalies.append(anomaly)

        return anomalies

    def get_analytics_summary(self) -> dict[str, Any]:
        """Get summary of analytics data."""
        return {
            'session_id': self.session_id,
            'total_metrics': len(self.metrics_history),
            'trends_analyzed': len(self.trends),
            'anomalies_detected': len(self.anomalies),
            'insights_generated': len(self.insights),
            'recent_insights': [asdict(insight) for insight in sorted(self.insights, key=lambda x: x.timestamp, reverse=True)[:5]],
            'critical_anomalies': len([a for a in self.anomalies if a.severity == 'critical']),
            'analytics_directory': str(self.analytics_dir)
        }

    def export_analytics_data(self, file_path: str | None = None) -> str:
        """Export analytics data to file."""
        if not file_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_path = str(self.analytics_dir / f"analytics_export_{self.session_id}_{timestamp}.json")

        export_data = {
            'session_id': self.session_id,
            'export_timestamp': datetime.now().isoformat(),
            'metrics_history': {k: [asdict(m) for m in v] for k, v in self.metrics_history.items()},
            'trends': {k: asdict(t) for k, t in self.trends.items()},
            'anomalies': [asdict(a) for a in self.anomalies],
            'insights': [asdict(i) for i in self.insights],
            'analytics_summary': self.get_analytics_summary()
        }

        import json
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, default=str)

        return file_path
