#!/usr/bin/env python3
"""Direct test of the intelligent research system without MCP wrapper.

This script directly tests the z-playground1 intelligence functions
without the @tool decorator complications.
"""

import asyncio
import os
import sys
from pathlib import Path

# Add current directory to path for imports
sys.path.append(os.path.dirname(__file__))

async def test_intelligent_research_directly():
    """Test the intelligent research system directly without MCP wrapper."""
    print("\n" + "=" * 80)
    print("ğŸ§ª DIRECT INTELLIGENT RESEARCH TEST")
    print("=" * 80)
    print("Testing z-playground1 intelligence without MCP constraints")

    try:
        # Import the core functions directly
        from utils.serp_search_utils import serp_search_and_extract
        print("âœ… serp_search_and_extract imported successfully")
    except ImportError as e:
        print(f"âŒ Failed to import serp_search_and_extract: {e}")
        return False

    try:
        # Import crawl4ai utilities to verify they work
        from utils.crawl4ai_utils import crawl_multiple_urls_with_cleaning
        print("âœ… crawl4ai_utils imported successfully")
    except ImportError as e:
        print(f"âŒ Failed to import crawl4ai_utils: {e}")
        return False

    # Test configuration
    kevin_dir = Path.home() / "lrepos" / "claude-agent-sdk-python" / "KEVIN"
    test_session_id = "direct-test-intelligent-research"
    test_query = "latest military activities on both sides in the Russia Ukraine war"

    print(f"\nğŸ”¬ Testing with query: '{test_query}'")
    print(f"ğŸ“ Output directory: {kevin_dir}")
    print(f"ğŸ†” Session ID: {test_session_id}")

    try:
        print("\nğŸš€ Starting intelligent research...")

        # This should use the advanced_content_extraction we integrated
        result = await serp_search_and_extract(
            query=test_query,
            search_type="search",
            num_results=15,  # Use z-playground1 default
            auto_crawl_top=8,   # Use z-playground1 default
            crawl_threshold=0.3,  # Use z-playground1 default
            session_id=test_session_id,
            kevin_dir=kevin_dir
        )

        print("\nğŸ“Š Results Analysis:")
        print(f"  - Result length: {len(result)} characters")
        print(f"  - Success: {'âœ… Content extracted' if len(result) > 1000 else 'âŒ No content extracted'}")

        # Check for content extraction indicators
        has_crawled_content = "EXTRACTED CONTENT" in result
        has_work_product = "Work Product Saved" in result
        has_multiple_sources = len(result.split('\n#')) > 5

        print(f"  - Has crawled content: {'âœ…' if has_crawled_content else 'âŒ'}")
        print(f"  - Has work products: {'âœ…' if has_work_product else 'âŒ'}")
        print(f"  - Multiple sources: {'âœ…' if has_multiple_sources else 'âŒ'}")

        # Show first 800 characters
        print("\nğŸ“„ Content Preview (first 800 chars):")
        print(result[:800])
        print("...")

        # Check work product directory
        work_product_dir = kevin_dir / "work_products" / test_session_id
        if work_product_dir.exists():
            work_files = list(work_product_dir.glob("*.md"))
            print(f"\nğŸ’¾ Work Product Files: {len(work_files)}")
            for work_file in work_files[:3]:
                file_size = work_file.stat().st_size
                print(f"  - {work_file.name} ({file_size:,} bytes)")
        else:
            print(f"\nâš ï¸  No work product directory found at: {work_product_dir}")

        if len(result) > 2000 and has_crawled_content:
            print("\nğŸ‰ SUCCESS: Intelligent research system working correctly!")
            print("âœ… Content length indicates successful extraction")
            print("âœ… 'EXTRACTED CONTENT' indicates advanced scraping worked")
            print("âœ… System ready for agent use")
            return True
        else:
            print("\nâš ï¸  Limited success detected")
            print("ğŸ” System may still have issues to resolve")
            return False

    except Exception as e:
        print(f"\nâŒ Error in intelligent research: {e}")
        print("\nCommon issues:")
        print("  - Check SERPER_API_KEY in .env file")
        print("  - Check OPENAI_API_KEY in .env file (for content cleaning)")
        print("  - Verify crawl4ai installation")
        return False


async def verify_crawl4ai_availability():
    """Verify Crawl4AI is properly installed."""
    print("\n" + "=" * 80)
    print("ğŸ” CRAWL4AI AVAILABILITY CHECK")
    print("=" * 80)

    try:
        # Test basic import
        from utils.crawl4ai_utils import SimpleCrawler
        print("âœ… SimpleCrawler imported successfully")

        # Check if we can create an instance
        crawler = SimpleCrawler()
        print("âœ… SimpleCrawler instance created successfully")

        # Check if we can access browser automation
        try:
            # Just test that the import works, no actual crawling
            from crawl4ai import AsyncWebCrawler
            print("âœ… AsyncWebCrawler available")
        except ImportError:
            print("âŒ AsyncWebCrawler not available")
            print("   Try: pip install crawl4ai playwright")
            print("   Then: playwright install chromium")
            return False

        print("âœ… Crawl4AI is properly installed and ready")
        return True

    except ImportError as e:
        print(f"âŒ Crawl4AI not available: {e}")
        print("\nTo install Crawl4AI:")
        print("  pip install crawl4ai playwright")
        print("  playwright install")
        print("  # Then try again")
        return False


async def run_direct_tests():
    """Run all direct tests."""
    print("\n" + "=" * 80)
    print("ğŸ§ª DIRECT INTELLIGENT RESEARCH SYSTEM TESTS")
    print("=" * 80)

    # Test 1: Verify Crawl4AI availability
    print("\n" + "=" * 20 + " STEP 1: CRAWL4AI CHECK " + "=" * 20)
    crawl4ai_available = await verify_crawl4ai_availability()

    if not crawl4ai_available:
        print("\nâŒ Cannot proceed without Crawl4AI")
        return False

    # Test 2: Test intelligent research
    print("\n" + "=" * 20 + " STEP 2: INTELLIGENT RESEARCH " + "=" * 20)
    research_success = await test_intelligent_research_directly()

    # Summary
    print("\n" + "=" * 80)
    print("ğŸ“Š TEST SUMMARY")
    print("=" * 80)

    results = [
        ("Crawl4AI Installation", crawl4ai_available),
        ("Intelligent Research", research_success)
    ]

    passed = sum(1 for _, result in results if result)
    total = len(results)

    for name, result in results:
        status = "âœ… PASSED" if result else "âŒ FAILED"
        print(f"  {name}: {status}")

    print(f"\nOverall: {passed}/{total} tests passed")

    if passed == total:
        print("\nğŸ‰ âœ… ALL TESTS PASSED!")
        print("âœ… Z-Playground1 intelligent research system is working")
        print("âœ… Ready to use with agents (when MCP wrapper is resolved)")
        print("âœ… Advanced scraping and content cleaning implemented")
        print("=" * 80)
        return True
    else:
        print(f"\nâš ï¸  {total - passed} test(s) failed")
        print("ğŸ”§ Check the failed components before agent integration")
        print("=" * 80)
        return False


if __name__ == "__main__":
    success = asyncio.run(run_direct_tests())
    exit(0 if success else 1)
